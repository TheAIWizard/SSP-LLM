apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-gguf-api
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-gguf-api
  template:
    metadata:
      labels:
        app: my-gguf-api
    spec:
      initContainers:
        - name: init-container
          image: busybox:latest
          command: ["/bin/sh", "-c"]
          args: ["curl -O /data/zephyr-7b-beta.Q8_0.gguf https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q8_0.gguf"] # a refaire plus propre avec configmap
          volumeMounts:
          - name: load-model
            mountPath: /data
      containers:
        - name: gguf-container
          image: ghcr.io/abetlen/llama-cpp-python:latest
          ports:
            - containerPort: 7860
          env:
            - name: MODEL
              value: "/app/data/zephyr-7b-beta.Q8_0.gguf"
            - name: N_GPU_LAYERS
              value: "-1"
            - name: N_CTX
              value: "4096"
            - name: N_BATCH
              value: "126"
          volumeMounts:
            - name: load-model
              mountPath: /data
              subPath: portal
          resources:
            limits:
              nvidia.com/gpu: "1"
              memory: "36Gi"
              cpu: "10000m"
      volumes:
        - name: load-model
          emptyDir: {}